{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GARVIT\\AppData\\Local\\Temp\\ipykernel_40644\\591221765.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers as keras_layers, Model, losses, optimizers, metrics\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MovieLens 20M dataset\n",
    "ratings_df = pd.read_csv(r'C:\\Users\\GARVIT\\Downloads\\archive\\rating.csv')\n",
    "movies_df = pd.read_csv(r'C:\\Users\\GARVIT\\Downloads\\archive\\movie.csv')\n",
    "tags_df = pd.read_csv(r'C:\\Users\\GARVIT\\Downloads\\archive\\tag.csv')\n",
    "\n",
    "# Merge ratings, movies, and tags dataframes\n",
    "data_df = pd.merge(ratings_df, movies_df[['movieId', 'title', 'genres']], left_on='movieId', right_on='movieId')\n",
    "data_df = pd.merge(data_df, tags_df[['userId', 'movieId', 'tag']], left_on=['userId', 'movieId'], right_on=['userId', 'movieId'], how='left')\n",
    "\n",
    "# Preprocess data\n",
    "user_ids = data_df['userId'].unique()\n",
    "movie_ids = data_df['movieId'].unique()\n",
    "\n",
    "user_id_map = {user_id: index for index, user_id in enumerate(user_ids)}\n",
    "movie_id_map = {movie_id: index for index, movie_id in enumerate(movie_ids)}\n",
    "\n",
    "data_df['userId'] = data_df['userId'].map(user_id_map)\n",
    "data_df['movieId'] = data_df['movieId'].map(movie_id_map)\n",
    "\n",
    "# Extract movie genres and tags\n",
    "mlb = MultiLabelBinarizer()\n",
    "genre_features = mlb.fit_transform(data_df['genres'].str.split('|'))\n",
    "\n",
    "tags_df = data_df.groupby('movieId')['tag'].apply(lambda x: ' '.join(x.fillna('').astype(str))).reset_index()\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tag_features = tfidf_vectorizer.fit_transform(tags_df['tag'])\n",
    "\n",
    "# Reduce dimensionality of genre features\n",
    "svd = TruncatedSVD(n_components=10, random_state=42)\n",
    "genre_features_reduced = svd.fit_transform(genre_features)\n",
    "\n",
    "# Reduce dimensionality of tag features\n",
    "svd = TruncatedSVD(n_components=10, random_state=42)\n",
    "tag_features_reduced = svd.fit_transform(tag_features)\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_df, test_df = train_test_split(data_df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator function\n",
    "def data_generator(df, genre_features, tag_features, batch_size=64):\n",
    "    def generator():\n",
    "        for i in range(0, len(df), batch_size):\n",
    "            batch_df = df.iloc[i:i+batch_size]\n",
    "\n",
    "            X = [\n",
    "                batch_df['userId'].values.reshape(-1, 1), \n",
    "                batch_df['movieId'].values.reshape(-1, 1), \n",
    "                genre_features[batch_df['movieId']],\n",
    "                tag_features[batch_df['movieId']]\n",
    "            ]\n",
    "            y = batch_df['rating'].values.astype('float32')\n",
    "\n",
    "            yield tuple(X), y\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_signature=(\n",
    "            (\n",
    "                tf.TensorSpec(shape=(None, 1), dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(None, 1), dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(None, 10), dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(None, 10), dtype=tf.float32)\n",
    "            ),\n",
    "            tf.TensorSpec(shape=(None,), dtype=tf.float32)\n",
    "        )\n",
    "    ).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "# Create NCF model\n",
    "num_users = len(user_ids)\n",
    "num_movies = len(movie_ids)\n",
    "num_genres = genre_features_reduced.shape[1]\n",
    "num_tags = tag_features_reduced.shape[1]\n",
    "\n",
    "def create_ncf_model(num_users, num_movies, num_genres, num_tags, embed_size=64, layer_sizes=[64, 32, 16]):\n",
    "    # Input layers\n",
    "    user_input = keras_layers.Input(shape=(1,), name='user_input')\n",
    "    movie_input = keras_layers.Input(shape=(1,), name='movie_input')\n",
    "    genre_input = keras_layers.Input(shape=(num_genres,), name='genre_input')\n",
    "    tag_input = keras_layers.Input(shape=(num_tags,), name='tag_input')\n",
    "    \n",
    "    # Embedding layers\n",
    "    user_embedding = keras_layers.Embedding(input_dim=num_users, output_dim=embed_size)(user_input)\n",
    "    movie_embedding = keras_layers.Embedding(input_dim=num_movies, output_dim=embed_size)(movie_input)\n",
    "    \n",
    "    # Flatten embedding layers\n",
    "    user_flatten = keras_layers.Flatten()(user_embedding)\n",
    "    movie_flatten = keras_layers.Flatten()(movie_embedding)\n",
    "    \n",
    "    # Concatenate embeddings, genre input, and tag input\n",
    "    concat_features = keras_layers.Concatenate()([user_flatten, movie_flatten, genre_input, tag_input])\n",
    "    \n",
    "    # Fully connected layers\n",
    "    for units in layer_sizes:\n",
    "        concat_features = keras_layers.Dense(units, activation='relu')(concat_features)\n",
    "    \n",
    "    # Output layer\n",
    "    output = keras_layers.Dense(1)(concat_features)\n",
    "    \n",
    "    # Create and compile model\n",
    "    model = Model(inputs=[user_input, movie_input, genre_input, tag_input], outputs=output)\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=0.001), loss=losses.MeanSquaredError(), metrics=[metrics.RootMeanSquaredError()])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m253320/253320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18311s\u001b[0m 72ms/step - loss: 0.7917 - root_mean_squared_error: 0.8877 - val_loss: 0.6853 - val_root_mean_squared_error: 0.8278\n",
      "Epoch 2/5\n",
      "\u001b[1m253320/253320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1us/step - loss: 0.1933 - root_mean_squared_error: 0.6218 - val_loss: 0.4234 - val_root_mean_squared_error: 0.9202\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\GARVIT\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m253320/253320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10051s\u001b[0m 40ms/step - loss: 0.6657 - root_mean_squared_error: 0.8159 - val_loss: 0.6612 - val_root_mean_squared_error: 0.8131\n",
      "Epoch 4/5\n",
      "\u001b[1m253320/253320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step - loss: 0.2381 - root_mean_squared_error: 0.6901 - val_loss: 0.3342 - val_root_mean_squared_error: 0.8176\n",
      "Epoch 5/5\n",
      "\u001b[1m253320/253320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17841s\u001b[0m 70ms/step - loss: 0.6246 - root_mean_squared_error: 0.7903 - val_loss: 0.6542 - val_root_mean_squared_error: 0.8088\n",
      "\u001b[1m63330/63330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 1ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [4053125, 4053120]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 52\u001b[0m\n\u001b[0;32m     49\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m ncf_model\u001b[38;5;241m.\u001b[39mpredict(test_gen, steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(test_df) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m batch_size)\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Calculate RMSE\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m rmse \u001b[38;5;241m=\u001b[39m sqrt(\u001b[43mmean_squared_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrating\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest RMSE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrmse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\GARVIT\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\GARVIT\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:497\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[1;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[0;32m    492\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m squared:\n\u001b[0;32m    493\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m root_mean_squared_error(\n\u001b[0;32m    494\u001b[0m             y_true, y_pred, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, multioutput\u001b[38;5;241m=\u001b[39mmultioutput\n\u001b[0;32m    495\u001b[0m         )\n\u001b[1;32m--> 497\u001b[0m y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    500\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    501\u001b[0m output_errors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage((y_true \u001b[38;5;241m-\u001b[39m y_pred) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, weights\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "File \u001b[1;32mc:\\Users\\GARVIT\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:102\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[1;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_reg_targets\u001b[39m(y_true, y_pred, multioutput, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     69\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same regression task.\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m        correct keyword.\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m     \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m check_array(y_true, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    104\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m check_array(y_pred, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[1;32mc:\\Users\\GARVIT\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:457\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    455\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    459\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    460\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [4053125, 4053120]"
     ]
    }
   ],
   "source": [
    "# Create and train NCF model using data generators\n",
    "batch_size = 64\n",
    "train_gen = data_generator(train_df, genre_features_reduced, tag_features_reduced, batch_size=batch_size)\n",
    "steps_per_epoch = len(train_df) // batch_size\n",
    "\n",
    "test_gen = data_generator(test_df, genre_features_reduced, tag_features_reduced, batch_size=batch_size)\n",
    "validation_steps = len(test_df) // batch_size\n",
    "\n",
    "# Create datasets using the data_generator function\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(train_df, genre_features_reduced, tag_features_reduced, batch_size=batch_size),\n",
    "    output_signature=(\n",
    "        (\n",
    "            tf.TensorSpec(shape=(None, 1), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(None, 1), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(None, 10), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(None, 10), dtype=tf.float32)\n",
    "        ),\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.float32)\n",
    "    )\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(test_df, genre_features_reduced, tag_features_reduced, batch_size=batch_size),\n",
    "    output_signature=(\n",
    "        (\n",
    "            tf.TensorSpec(shape=(None, 1), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(None, 1), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(None, 10), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(None, 10), dtype=tf.float32)\n",
    "        ),\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.float32)\n",
    "    )\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Create and train NCF model\n",
    "ncf_model = create_ncf_model(num_users, num_movies, num_genres, num_tags)\n",
    "\n",
    "ncf_model.fit(\n",
    "    train_dataset,\n",
    "    epochs=5,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=test_dataset,\n",
    "    validation_steps=validation_steps\n",
    ")\n",
    "\n",
    "\n",
    "# Predict ratings on test data\n",
    "y_pred = ncf_model.predict(test_gen, steps=len(test_df) // batch_size).flatten()\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = sqrt(mean_squared_error(test_df['rating'].values, y_pred))\n",
    "print(f'Test RMSE: {rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63330/63330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 1ms/step\n",
      "Test RMSE: 0.8088421623179642\n"
     ]
    }
   ],
   "source": [
    "# Predict ratings on test data\n",
    "y_pred = ncf_model.predict(test_gen, steps=len(test_df) // batch_size).flatten()\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = sqrt(mean_squared_error(test_df['rating'].values[:len(y_pred)], y_pred))\n",
    "print(f'Test RMSE: {rmse}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
